<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Haoran Xu | Homepage of Haoran Xu - Xidian University</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Haoran Xu" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Homepage of Haoran Xu - Xidian University" />
<meta property="og:description" content="Homepage of Haoran Xu - Xidian University" />
<link rel="canonical" href="/" />
<meta property="og:url" content="/" />
<meta property="og:site_name" content="Hoyin" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Haoran Xu" />
<script type="application/ld+json">
{"headline":"Haoran Xu","@type":"WebSite","url":"/","description":"Homepage of Haoran Xu - Xidian University","name":"Haoran Xu","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="Haoran Xu" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Haoran Xu</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/pubs/">Publications</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <p><img src="/image.jpg" alt="abc" style="float: right;margin-right: 7px;margin-top: 7px;height: 200px;border: 5" /></p>

<p>Hello! I am a third year Master student at Xidian University, advised by Prof. <a href="http://urban-computing.com/yuzheng"><strong>Zheng Yu</strong></a>. My research topic lies in the field of Deep Learning and Reinforcement Learning, especially offline (batch) RL and its applications. I am thrilled in building AI agents that can learn purely from data and make reliable and effective decisions in real-world applications.</p>

<p>I previously interned at <strong>Microsoft Research Lab Asia (MSRA)</strong> during Summer 2018 and <strong>JD Technology</strong> from 2019.3 to 2021.6 on Urban Computing.</p>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="Aviral Kumar" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Aviral Kumar</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/pubs/">Publications</a><a class="page-link" href="/contact/">Contact</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Publications</h1>
  </header>

  <div class="post-content">
    <p><strong>Publications</strong></p>

<ul>
  <li>
    <p><strong>Implicit Under-Parameterization Inhibts Data-Efficient Deep Reinforcement Learning</strong><br />
<strong><em>Aviral Kumar^</em></strong><em>, Rishabh Agarwal^</em>, Dibya Ghosh, Sergey Levine	<a href="https://arxiv.org/abs/2010.14498">[arXiv]</a><br />
<em>International Conference on Learning Representations (</em><strong>ICLR</strong><em>), 2021</em></p>
  </li>
  <li>
    <p><strong>OPAL: Offline Primitive Discovery For Accelerating Reinforcement Learning</strong><br />
Anurag Ajay, <strong><em>Aviral Kumar</em></strong>, Pulkit Agarwal, Sergey Levine, Ofir Nachum	<a href="https://arxiv.org/abs/2010.13611">[arXiv]</a><br />
<em>International Conference on Learning Representations (</em><strong>ICLR</strong><em>), 2021</em></p>
  </li>
  <li>
    <p><strong>Conservative Safety Crtics for Exploration</strong><br />
Homanaga Bharadhwaj, <strong><em>Aviral Kumar</em></strong>, Nicholas Rhinehart, Sergey Levine, Florian Shkruti, Animesh Garg 	<a href="https://arxiv.org/abs/2010.14497">[arXiv]</a> <br />
<em>International Conference on Learning Representations (</em><strong>ICLR</strong><em>), 2021</em></p>
  </li>
  <li>
    <p><strong>COG: Connecting New Skills to Past Experience with Offline Reinforcement Learning</strong><br />
Avi Singh, Albert Yu, Jonathan Yang, Jesse Zhang, <strong><em>Aviral Kumar</em></strong>, Sergey Levine<br />
<a href="https://arxiv.org/abs/2010.14500">[arXiv]</a> <a href="https://sites.google.com/view/cog-rl">[website]</a> (October 2020)<br />
<em>4th Conference on Robot Learning (</em><strong>CoRL</strong><em>), 2020</em><br />
In this work, we show how the simple conservative Q-learning (CQL) algorithm for offline RL can allow us effectively leverage prior, task-agnositc robotic interaction datasets to learn complex policies to solve novel downstream tasks, from a wide variety of initial conditions, using only minimal supervision from the downstream task, which is also labeled with only a 0-1 sparse reward. No need for hierarchies, skills or primitives.</p>
  </li>
  <li>
    <p><strong>Conservative Q-Learning for Offline Reinforcement Learning</strong><br />
<strong><em>Aviral Kumar</em></strong>, Aurick Zhou, George Tucker, Sergey Levine<br />
<a href="https://arxiv.org/abs/2006.04779">[arXiv]</a> <a href="https://sites.google.com/view/cql-offline-rl">[website]</a> <a href="https://drive.google.com/file/d/10ImlQ0EyPzrtTrXsrxrZVxMhXF-rxuFe/view?usp=sharing">[talk]</a> (June 2020)<br />
<em>Advances in Neural Information Processing Systems (</em><strong>NeurIPS</strong><em>), 2020</em><br />
In this work we present a novel offline RL algorithm that learns a Q-function that lower-bounds the policy value, by adding specific regularizers to Q-function training. By optimizing the policy against this conservative Q-function, we are able to obtain state-of-the-art performance on a number of hard control tasks for offline RL and Atari games with limited data.</p>
  </li>
  <li>
    <p><strong>DisCor: Corrective Feedback in Reinforcement Learning via Distribution Correction</strong><br />
<strong><em>Aviral Kumar</em></strong>, Abhishek Gupta, Sergey Levine       <a href="https://arxiv.org/abs/2003.07305">[arXiv]</a> <a href="https://bair.berkeley.edu/blog/2020/03/16/discor/">[BAIR Blog]</a> (March 2020) <br />
<em>Advances in Neural Information Processing Systems (</em><strong>NeurIPS</strong><em>) (Spotlight Presentation, 3% acceptance rate), 2020</em><br />
In this paper, we ask the following question: Which distribution should be used to train Q-functions? We show that the choice of data distribution affects the performance of deep RL algorithms. Contrary to what is commonly believed, even on-policy data collection may fail to correct errors during Q-learning. We then devise a method for optimizing data distributions based on maximizing error reduction.</p>
  </li>
  <li>
    <p><strong>Model Inversion Networks for Model-Based Optimization</strong><br />
<strong><em>Aviral Kumar</em></strong>, Sergey Levine  <a href="https://arxiv.org/abs/1912.13464">[arXiv]</a> (December 2019)<br />
<em>Advances in Neural Information Processing Systems (</em><strong>NeurIPS</strong><em>), 2020</em><br />
Model-based optimization problems or black-box optimization problems appear in several scenarios, such as protein design, or aircraft design or contextual bandits. In this work, we design a new model-based optimization method for extremely high-dimensional problems (&gt;1000 dims) which is robust to going off the manifold of valid inputs.</p>
  </li>
  <li>
    <p><strong>One Solution is Not All You Need: Few-Shot Extrapolation Via Structured MaxEnt RL</strong><br />
Saurabh Kumar, <strong><em>Aviral Kumar</em></strong>, Sergey Levine, Chelsea Finn  <a href="https://arxiv.org/pdf/2010.14484.pdf">[arXiv]</a> (October 2020)<br />
<em>Advances in Neural Information Processing Systems (</em><strong>NeurIPS</strong><em>), 2020</em><br />
In this paper, we propose to utilize “structured” diversity to obtain multiple different ways of solving a given task, allowing us to revert to a different way of solvcing a task for environment perturbations ina few-shot extrapolation scenario.</p>
  </li>
  <li>
    <p><strong>Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction</strong><br />
 <strong><em>Aviral Kumar*</em></strong>, Justin Fu*, George Tucker, Sergey Levine<br />
 <em>Advances in Neural Information Processing Systems (</em><strong>NeurIPS</strong><em>), 2019</em> <br />
 <a href="https://arxiv.org/abs/1906.00949">[Paper]</a>
 <a href="https://sites.google.com/view/bear-off-policyrl/home">[Project Page]</a> <a href="https://bair.berkeley.edu/blog/2019/12/05/bear/">[BAIR Blog]</a> \
 In this paper, we attribute the problem of optimization of Q-functions in offline reinforcement learning settings to querying the Q-function at out-of-distribution action inputs during training. We propose a simple fix: constrain the support of the learned policy to lie within the support of the behavior policy and use this to perform offline actor-critic or Q-learning.</p>
  </li>
  <li>
    <p><strong>Diagnosing Bottlenecks in Deep Q-Learning Algorithms</strong><br />
  Justin Fu*, <strong><em>Aviral Kumar*</em></strong>, Matthew Soh, Sergey Levine<br />
  <em>36th International Conference on Machine Learning (</em><strong>ICML</strong><em>) 2019</em>
  <a href="https://arxiv.org/abs/1902.10250">[Paper]</a> (* Equal Contribution)<br />
  In this paper, we provide an empirical analysis of some of the design factors in deep Q-learning algorithms including the effects of the capacity of the function approximator, the effect of sampling error and the effect of training distribution. We also devise a set of toy gridworld environments that test these properties and can be used for prototyping new algorithms.</p>
  </li>
  <li>
    <p><strong>Graph Normalizing Flows</strong><br />
Jenny Liu*, <strong><em>Aviral Kumar*</em></strong>, Jimmy Ba, Jamie Kiros, Kevin Swersky<br />
<em>Advances in Neural Information Processing Systems (</em><strong>NeurIPS</strong><em>), 2019</em> <br />
<a href="https://arxiv.org/abs/1905.13177">[Paper]</a>
<a href="https://sites.google.com/view/graph-normalizing-flows/home">[Project Page]</a> (* Equal Contribution)</p>
  </li>
  <li>
    <p><strong>Trainable Calibration Measures for Neural Networks from Kernel Mean Embeddings</strong><br />
  <strong><em>Aviral Kumar</em></strong>, Sunita Sarawagi, Ujjwal Jain<br />
  <em>35th International Conference on Machine Learning (</em><strong>ICML</strong><em>) 2018</em> <a href="http://proceedings.mlr.press/v80/kumar18a.html">[Main Paper]</a></p>
  </li>
  <li>
    <p><strong>GRevnet: Improving Graph Neural Networks with Reversible Computation</strong><br />
  <strong><em>Aviral Kumar</em></strong>, Jimmy Ba, Jamie Kiros, Kevin Swersky<br />
  NeuRIPS 2018 Relational Representation Learning Workshop <a href="https://drive.google.com/file/d/1UYsTSnyKjl6MAox9vwGtV77wB_3vMavR/view">[Paper]</a></p>
  </li>
  <li>
    <p><strong>Feudal Learning for Large Discrete Action Spaces with Recursive Substructure</strong><br />
  <strong><em>Aviral Kumar</em></strong>, Kevin Swersky, Geoffrey Hinton<br />
  Hierarchical Reinforcement Learning Workshop, NIPS 2017<br />
  <a href="https://drive.google.com/open?id=1ddaP7FudvLuI6Yqk6B3qneigP_pQiVRD">[Main Paper]</a></p>
  </li>
</ul>




  </div>

</article>


      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Haoran Xu</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Haoran Xu</li><li><a class="u-email" href="mailto:ryanxhr@gmail.com">ryanxhr@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/ryanxhr"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">ryanxhr</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Homepage of Haoran Xu - Xidian University (the website templete is taken from Aviral Kumar)</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
